{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분석의 장벽을 낮추는 판다스 핀셋 강의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LG 에러 탐지 대회 코드 리뷰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library & Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, precision_recall_curve, recall_score, precision_score\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.3\n",
      "1.0.5\n",
      "0.23.2\n",
      "3.0.0\n",
      "0.24.1\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(lightgbm.__version__)\n",
    "print(catboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_P_PATH = r'C:\\Users\\Wyatt\\wyatt37/Data/systemError/train_problem_data.csv'\n",
    "TRAIN_Q_PATH = r'C:\\Users\\Wyatt\\wyatt37/Data/systemError/train_quality_data.csv'\n",
    "TRAIN_E_PATH = r'C:\\Users\\Wyatt\\wyatt37/Data/systemError/train_err_data.csv'\n",
    "TEST_Q_PATH = r'C:\\Users\\Wyatt\\wyatt37/Data/systemError/test_quality_data.csv'\n",
    "TEST_E_PATH = r'C:\\Users\\Wyatt\\wyatt37/Data/systemError/test_err_data.csv'\n",
    "SUBMISSION_PATH = r'C:\\Users\\Wyatt\\wyatt37/Data/systemError/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_p = pd.read_csv(TRAIN_P_PATH)\n",
    "train_q = pd.read_csv(TRAIN_Q_PATH)\n",
    "train_e = pd.read_csv(TRAIN_E_PATH)\n",
    "test_q = pd.read_csv(TEST_Q_PATH)\n",
    "test_e = pd.read_csv(TEST_E_PATH)\n",
    "submission = pd.read_csv(SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5429, 2),\n",
       " (828624, 16),\n",
       " (16554663, 6),\n",
       " (747972, 16),\n",
       " (16532648, 6),\n",
       " (14999, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p.shape, train_q.shape, train_e.shape, test_q.shape, test_e.shape, submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5429 entries, 0 to 5428\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   user_id  5429 non-null   int64\n",
      " 1   time     5429 non-null   int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 85.0 KB\n"
     ]
    }
   ],
   "source": [
    "train_p.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 828624 entries, 0 to 828623\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   time        828624 non-null  int64  \n",
      " 1   user_id     828624 non-null  int64  \n",
      " 2   fwver       788544 non-null  object \n",
      " 3   quality_0   684192 non-null  float64\n",
      " 4   quality_1   828624 non-null  int64  \n",
      " 5   quality_2   788511 non-null  float64\n",
      " 6   quality_3   828624 non-null  int64  \n",
      " 7   quality_4   828624 non-null  int64  \n",
      " 8   quality_5   828604 non-null  object \n",
      " 9   quality_6   828624 non-null  int64  \n",
      " 10  quality_7   828624 non-null  object \n",
      " 11  quality_8   828624 non-null  object \n",
      " 12  quality_9   828624 non-null  object \n",
      " 13  quality_10  828624 non-null  object \n",
      " 14  quality_11  828624 non-null  int64  \n",
      " 15  quality_12  828624 non-null  int64  \n",
      "dtypes: float64(2), int64(8), object(6)\n",
      "memory usage: 101.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train_q.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16554663 entries, 0 to 16554662\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Dtype \n",
      "---  ------    ----- \n",
      " 0   user_id   int64 \n",
      " 1   time      int64 \n",
      " 2   model_nm  object\n",
      " 3   fwver     object\n",
      " 4   errtype   int64 \n",
      " 5   errcode   object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 757.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_e.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 747972 entries, 0 to 747971\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   time        747972 non-null  int64  \n",
      " 1   user_id     747972 non-null  int64  \n",
      " 2   fwver       725208 non-null  object \n",
      " 3   quality_0   641388 non-null  float64\n",
      " 4   quality_1   747961 non-null  object \n",
      " 5   quality_2   726857 non-null  float64\n",
      " 6   quality_3   747972 non-null  int64  \n",
      " 7   quality_4   747972 non-null  int64  \n",
      " 8   quality_5   747928 non-null  object \n",
      " 9   quality_6   747972 non-null  int64  \n",
      " 10  quality_7   747972 non-null  object \n",
      " 11  quality_8   747972 non-null  object \n",
      " 12  quality_9   747972 non-null  object \n",
      " 13  quality_10  747972 non-null  object \n",
      " 14  quality_11  747972 non-null  int64  \n",
      " 15  quality_12  747972 non-null  int64  \n",
      "dtypes: float64(2), int64(7), object(7)\n",
      "memory usage: 91.3+ MB\n"
     ]
    }
   ],
   "source": [
    "test_q.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16532648 entries, 0 to 16532647\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Dtype \n",
      "---  ------    ----- \n",
      " 0   user_id   int64 \n",
      " 1   time      int64 \n",
      " 2   model_nm  object\n",
      " 3   fwver     object\n",
      " 4   errtype   int64 \n",
      " 5   errcode   object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 756.8+ MB\n"
     ]
    }
   ],
   "source": [
    "test_e.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_problem(df, object_='binary'):\n",
    "    \"\"\"\n",
    "    definition:\n",
    "    train_problem 테이블을 받아서 target 값으로 변환\n",
    "    1. {0, 1}의 binary로 변환\n",
    "    2. {0 ~ n}의 multiclass로 변환\n",
    "    \"\"\"\n",
    "    \n",
    "    # 10001부터 24999까지의 index를 만들어줍니다.\n",
    "    user_id_idx = np.array(range(10000, 25000, 1))\n",
    "    \n",
    "    # train_new_p라는 새로운 df를 만들고 index는 위에서 만든 user_id_idx 로 지정해줍니다.\n",
    "    new_p = pd.DataFrame(index = user_id_idx)\n",
    "    new_p['target'] = 0\n",
    "    \n",
    "    if object_ == 'binary':\n",
    "        new_p.iloc[df.user_id.unique()-10000] = 1\n",
    "        new_p = new_p.reset_index()\n",
    "        new_p.rename({'index':'user_id'}, axis=1, inplace=True)\n",
    "        \n",
    "    elif object_ == 'multi':\n",
    "        # multi는 count()로 집계를 해줍니다.\n",
    "        new_p['target'] = df.groupby('user_id')['time'].count()\n",
    "        new_p = new_p.fillna(0)\n",
    "        new_p = new_p.reset_index()\n",
    "        new_p.rename({'index':'user_id'}, axis=1, inplace=True)\n",
    "        \n",
    "    return new_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_b_p = preprocessing_problem(train_p, 'binary')\n",
    "train_m_p = preprocessing_problem(train_p, 'multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_quality(df):\n",
    "    \"\"\"\n",
    "    definition:\n",
    "    EDA를 통해 알아낸 정보로 train_q, test_q를 정리해서 내뿜어줍니다.\n",
    "    1. qaulity_3, quality_4 를 drop 합니다.(단일 value)\n",
    "    2. qaulity_k 변수들을 정수로 encoding 합니다.\n",
    "    \"\"\"\n",
    "    # 먼저 3, 4번을 drop 합니다.\n",
    "    df.drop(['quality_3', 'quality_4'], axis=1, inplace=True)\n",
    "    \n",
    "    # qual 변수만 할당해주고, 정수로 형변환 해줍니다.\n",
    "    columns = train_q.columns[train_q.columns.str.contains('quality')]\n",
    "    # for문을 통해 각 column을 반복 작업해줍니다.\n",
    "    for col in columns:\n",
    "        if df[col].dtype == 'float64': # 기존에 float은 패스\n",
    "            df[col] = df[col].fillna(-2)\n",
    "        elif df[col].dtype == 'int64': # 기존에 int도 패스\n",
    "            df[col] = df[col].fillna(-2)\n",
    "            #print(col)\n",
    "        else:\n",
    "            # nan값이 있으면 float으로 갈 수 없으니 '-2' 으로 채워줍니다.\n",
    "            df[col] = df[col].fillna('-2')\n",
    "            df[col] = df[col].apply(lambda x: str(x).replace(',' , ''))\n",
    "            df[col] = df[col].astype(np.float64)\n",
    "        \n",
    "    # fwver 에서 null 값이 꽤 있습니다. missing으로 채우겠습니다.\n",
    "    df.fwver = df.fwver.fillna('missing')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q = preprocessing_quality(train_q)\n",
    "test_q = preprocessing_quality(test_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fwver(df):\n",
    "    \"\"\"\n",
    "    definition:\n",
    "    별건 아니고, e-set에 fwver 변수에서 '10' 이라는 값이 있는데, 이게 errtype이랑 겹쳐요.\n",
    "    그래서 10을 -> 8.5.2 으로 바꿔줄 겁니다.\n",
    "    굳이 이렇게 바꾸는 이유는, 해당 fw가 8.5.3버전과 같은 model_nm을 공유하기 때문입니다.\n",
    "    \"\"\"\n",
    "    df.fwver = df.fwver.replace('10', '8.5.2')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_e = preprocessing_fwver(train_e)\n",
    "test_e = preprocessing_fwver(test_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datetime(df):\n",
    "    \"\"\"\n",
    "    definition:\n",
    "    'time' column이 str로 되어 있으니, datetime으로 바꿔주는 함수입니다.\n",
    "    다만 'time'양식이 pandas함수에 적용이 안되니, 강제로 슬라이싱해서 만들어줘야 합니다.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df['year'] = df['time'].apply(lambda x: str(x)[:4])\n",
    "    df['month'] = df['time'].apply(lambda x: str(x)[4:6])\n",
    "    df['day'] = df['time'].apply(lambda x: str(x)[6:8])\n",
    "    df['hour'] = df['time'].apply(lambda x: str(x)[8:10])\n",
    "    df['minute'] = '00' # minute을 넣어주지 않으면 datetime이 완성이 안되니, 00으로 넣어줍니다.\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df.year + df.month + df.day + df.hour + df.minute)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_p = make_datetime(train_p)\n",
    "train_q = make_datetime(train_q)\n",
    "test_q = make_datetime(test_q)\n",
    "train_e = make_datetime(train_e)\n",
    "test_e = make_datetime(test_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Data Check & Missing Value Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_test_column_values(train, test, column):\n",
    "    # 함수 정의: 두 데이터 테이블과 특정 컬럼을 집어 넣으면 value를 비교하여 출력하는 함수\n",
    "    \n",
    "    # Train/Test set의 입력 칼럼의 value를 set으로 받아줍니다.\n",
    "    train_colset = set(train[column])\n",
    "    test_colset  = set(test[column])\n",
    "\n",
    "    # Train/Test-set 고유한 value의 개수를 구함.\n",
    "    print(f\"Train-set에 있는 고유한 value 개수 : {len(train_colset)}\")\n",
    "    print(f\"Test-set에 있는 고유한 value 개수 : {len(test_colset)}\")\n",
    "\n",
    "    # Train/Test-set 모두에 포함되어있는 value를 구함.\n",
    "    print('='* 80)\n",
    "    common_colset = train_colset.intersection(test_colset)\n",
    "    print(f\"Train/Test-set에 공통으로 포함되어 있는 value 개수 : {len(common_colset)}\")\n",
    "    if len(common_colset) > 100: # value가 너무 많으면 어차피 보기 힘드므로 출력을 pass 합니다\n",
    "        pass\n",
    "    else:\n",
    "        try: # int나 float은 sorted가 먹지 않기 때문에 try except로 넣어줍니다.\n",
    "            print(f\"Train/Test-set에 공통으로 포함되어 있는 value : {sorted(common_colset)}\")\n",
    "        except:\n",
    "            print(f\"Train/Test-set에 공통으로 포함되어 있는 value : {(common_colset)}\")\n",
    "\n",
    "    # Train-set에만 있는 value를 구함.\n",
    "    print('='* 80)\n",
    "    train_only_colset = train_colset.difference(test_colset)\n",
    "    print(f\"Train-set에만 있는 value는 총 {len(train_only_colset)} 개 입니다.\")\n",
    "    if len(train_only_colset) > 100:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"Train-set에만 있는 value는 : {sorted(train_only_colset)}\")\n",
    "        except:\n",
    "            print(f\"Train-set에만 있는 value는 : {(train_only_colset)}\")\n",
    "\n",
    "    # Test-set에만 있는 value를 구함.\n",
    "    print('='* 80)\n",
    "    test_only_colset = test_colset.difference(train_colset)\n",
    "    print(f\"Test-set에만 있는 value는 총 {len(test_only_colset)} 개 입니다.\")\n",
    "    if len(test_only_colset) > 100:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"Test-set에만 있는 value는 : {sorted(test_only_colset)}\")\n",
    "        except:\n",
    "            print(f\"Test-set에만 있는 value는 : {(test_only_colset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set에 있는 고유한 value 개수 : 37\n",
      "Test-set에 있는 고유한 value 개수 : 40\n",
      "================================================================================\n",
      "Train/Test-set에 공통으로 포함되어 있는 value 개수 : 31\n",
      "Train/Test-set에 공통으로 포함되어 있는 value : ['03.11.1141', '03.11.1149', '03.11.1167', '04.16.3439', '04.16.3553', '04.16.3569', '04.16.3571', '04.22.1656', '04.22.1666', '04.22.1684', '04.22.1750', '04.22.1778', '04.33.1125', '04.33.1149', '04.33.1171', '04.33.1185', '04.33.1261', '04.73.2237', '04.73.2571', '04.82.1684', '04.82.1730', '04.82.1778', '05.15.2092', '05.15.2114', '05.15.2120', '05.15.2138', '05.15.3104', '05.66.3237', '05.66.3571', '8.5.2', '8.5.3']\n",
      "================================================================================\n",
      "Train-set에만 있는 value는 총 6 개 입니다.\n",
      "Train-set에만 있는 value는 : ['04.16.2641', '04.16.3345', '04.22.1442', '04.33.1095', '05.15.2090', '05.15.2122']\n",
      "================================================================================\n",
      "Test-set에만 있는 value는 총 9 개 입니다.\n",
      "Test-set에만 있는 value는 : ['04.22.1170', '04.22.1448', '04.22.1478', '04.22.1608', '04.22.1772', '04.73.2569', '04.73.2577', '10.22.1770', '10.22.1780']\n"
     ]
    }
   ],
   "source": [
    "check_train_test_column_values(train_e, test_e, 'fwver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## errcode 탐험하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_e[['errtype', 'errcode']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 _unique:  ['0' 'P-44010' 'P-41011' 'P-41007 ' 'P-44010 ' 'P-41007' 'P-41011 '\n",
      " 'P-41001']\n",
      "2 _unique:  ['1' '0']\n",
      "3 _unique:  ['1' '2' '0']\n",
      "4 _unique:  ['0' '1']\n",
      "5 _unique:  ['B-A8002' 'Q-64002' 'S-61001' 'U-81009' 'V-21008' '700001' 'S-64002'\n",
      " 'J-30021' 'S-65002' 'Q-64001' 'H-51042' 'C-11017' 'H-51046' 'H-51049'\n",
      " 'V-21002' 'V-21003' 'V-21004' 'V-21005' 'B-51042' 'M-99999' 'U-82024'\n",
      " 'H-51048' '60045' 'U-82026' 'C-13053' 'C-14014' 'V-21010' 'J-20029'\n",
      " 'J-30010' 'Y-00008' 'S-64000' 'En00409' 'E-59902' 'Q-73004' 'C-12032'\n",
      " 'J-40011' 'U-82023' 'Q-73006' 'D-10011' 'S-65' 'M-51007' 'S-64001'\n",
      " 'Y-00005' 'P_41007' '2638' 'CM a' nan '40013' 'D-99999' 'U-82004'\n",
      " 'En00402' 'B-51049' 'C-11020' 'V-21007' 'M-51020' 'En00406' 'C-11087'\n",
      " 'U-81000' 'Y-00004' 'C-13039' '0001' 'U-82020' 'http' 'P_41001' 'U-81014']\n",
      "6 _unique:  ['1' '14']\n",
      "7 _unique:  ['1' '14']\n",
      "8 _unique:  ['PHONE_ERR' 'PUBLIC_ERR' '20']\n",
      "9 _unique:  ['V-21002' 'V-21005' '1' 'C-14014' 'V-21008' 'C-12032' 'V-21004' 'C-13039'\n",
      " 'C-11020']\n",
      "10 _unique:  ['1']\n",
      "11 _unique:  ['1']\n",
      "12 _unique:  ['1']\n",
      "13 _unique:  ['1']\n",
      "14 _unique:  ['1' '14' '13']\n",
      "15 _unique:  ['1']\n",
      "16 _unique:  ['1']\n",
      "17 _unique:  ['14' '13' '1' '21' '12']\n",
      "18 _unique:  ['1']\n",
      "19 _unique:  ['1']\n",
      "20 _unique:  ['1']\n",
      "21 _unique:  ['1']\n",
      "22 _unique:  ['1']\n",
      "23 _unique:  ['standby' 'active' 'connection timeout' 'terminate by peer user'\n",
      " 'connection fail to establish' 'connectionterminated by local host'\n",
      " 'UNKNOWN' 'connection fail for LMP response timout']\n",
      "24 _unique:  ['1']\n",
      "25 _unique:  ['2' 'scanning timeout' '1' 'UNKNOWN' 'terminate by peer user'\n",
      " 'connectionterminated by local host' 'connection timeout'\n",
      " 'connection fail to establish' 'L2CAP connection cancelled']\n",
      "26 _unique:  ['1']\n",
      "27 _unique:  ['1']\n",
      "28 _unique:  ['1']\n",
      "30 _unique:  ['4' '0' '1' '3' '2']\n",
      "31 _unique:  ['1' '0']\n",
      "32 _unique:  ['80' '79' '81' '86' '84' '77' '78' '85' '90' '89' '88' '83' '95' '94'\n",
      " '91' '87' '82' '93' '-269' '92' '96' '97' '100' '105' '110' '106' '101'\n",
      " '76' '104' '99' '109' '98' '75' '74' '108' '103' '42' '107' '102' '72'\n",
      " '41' '70' '71' '69' '73' '68' '62' '46' '38' '37' '-270' '47' '65' '67'\n",
      " '66']\n",
      "33 _unique:  ['2' '3' '1']\n",
      "34 _unique:  ['4' '1' '2' '3' '6' '5']\n",
      "35 _unique:  ['1']\n",
      "36 _unique:  ['8.0']\n",
      "37 _unique:  ['0' '1']\n",
      "38 _unique:  ['6796' '5738' '6467' ... '4526' '3965' '25999']\n",
      "39 _unique:  ['1' '0']\n",
      "40 _unique:  ['1' '0']\n",
      "41 _unique:  ['NFANDROID2']\n",
      "42 _unique:  ['3' '2']\n"
     ]
    }
   ],
   "source": [
    "count = {}\n",
    "\n",
    "for i in sorted(temp.errtype.unique()):\n",
    "\n",
    "    print(i, \"_unique: \", temp[temp.errtype == i]['errcode'].unique())\n",
    "    count[i] = (len(temp[temp.errtype == i]['errcode'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 _'s Unique User Counts:  950\n",
      "2 _'s Unique User Counts:  390\n",
      "3 _'s Unique User Counts:  1679\n",
      "4 _'s Unique User Counts:  6881\n",
      "5 _'s Unique User Counts:  11010\n",
      "6 _'s Unique User Counts:  6467\n",
      "7 _'s Unique User Counts:  7648\n",
      "8 _'s Unique User Counts:  64\n",
      "9 _'s Unique User Counts:  63\n",
      "10 _'s Unique User Counts:  2360\n",
      "11 _'s Unique User Counts:  14767\n",
      "12 _'s Unique User Counts:  14799\n",
      "13 _'s Unique User Counts:  3711\n",
      "14 _'s Unique User Counts:  7891\n",
      "15 _'s Unique User Counts:  14545\n",
      "16 _'s Unique User Counts:  14431\n",
      "17 _'s Unique User Counts:  5747\n",
      "18 _'s Unique User Counts:  1768\n",
      "19 _'s Unique User Counts:  419\n",
      "20 _'s Unique User Counts:  1546\n",
      "21 _'s Unique User Counts:  381\n",
      "22 _'s Unique User Counts:  10281\n",
      "23 _'s Unique User Counts:  10046\n",
      "24 _'s Unique User Counts:  2519\n",
      "25 _'s Unique User Counts:  420\n",
      "26 _'s Unique User Counts:  13026\n",
      "27 _'s Unique User Counts:  2267\n",
      "28 _'s Unique User Counts:  2275\n",
      "30 _'s Unique User Counts:  375\n",
      "31 _'s Unique User Counts:  11091\n",
      "32 _'s Unique User Counts:  1933\n",
      "33 _'s Unique User Counts:  11074\n",
      "34 _'s Unique User Counts:  3733\n",
      "35 _'s Unique User Counts:  2039\n",
      "36 _'s Unique User Counts:  8435\n",
      "37 _'s Unique User Counts:  8434\n",
      "38 _'s Unique User Counts:  1822\n",
      "39 _'s Unique User Counts:  766\n",
      "40 _'s Unique User Counts:  10462\n",
      "41 _'s Unique User Counts:  6142\n",
      "42 _'s Unique User Counts:  6382\n"
     ]
    }
   ],
   "source": [
    "err_type_user_counts = []\n",
    "for i in sorted(train_e.errtype.unique()):\n",
    "    print(i, \"_'s Unique User Counts: \", train_e[train_e.errtype == i].groupby('user_id').count().shape[0])\n",
    "    err_type_user_counts.append(train_e[train_e.errtype == i].groupby('user_id').count().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quality log 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 nunique:  754\n",
      "1 번째 nunique:  31\n",
      "2 번째 nunique:  799\n",
      "5 번째 nunique:  4745\n",
      "6 번째 nunique:  549\n",
      "7 번째 nunique:  884\n",
      "8 번째 nunique:  42\n",
      "9 번째 nunique:  523\n",
      "10 번째 nunique:  4200\n",
      "11 번째 nunique:  12\n",
      "12 번째 nunique:  14\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    print(i,'번째 nunique: ' ,train_q['quality_{}'.format(i)].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 nunique:  541\n",
      "1 번째 nunique:  35\n",
      "2 번째 nunique:  597\n",
      "5 번째 nunique:  3995\n",
      "6 번째 nunique:  528\n",
      "7 번째 nunique:  839\n",
      "8 번째 nunique:  44\n",
      "9 번째 nunique:  431\n",
      "10 번째 nunique:  3515\n",
      "11 번째 nunique:  13\n",
      "12 번째 nunique:  17\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    print(i,'번째 nunique: ' ,test_q['quality_{}'.format(i)].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0    542790\n",
      "-2.0    144432\n",
      "-1.0    130828\n",
      " 1.0      2097\n",
      " 2.0      1252\n",
      " 3.0       518\n",
      " 4.0       410\n",
      " 5.0       385\n",
      " 6.0       358\n",
      " 7.0       314\n",
      "Name: quality_0, dtype: int64\n",
      " 0    670270\n",
      "-1    153649\n",
      " 1      2567\n",
      " 2      1140\n",
      " 3       391\n",
      " 4       202\n",
      " 5       134\n",
      " 6        71\n",
      " 7        50\n",
      " 8        35\n",
      "Name: quality_1, dtype: int64\n",
      " 0.0    632469\n",
      "-1.0    144392\n",
      "-2.0     40113\n",
      " 1.0      2937\n",
      " 2.0      1073\n",
      " 3.0       580\n",
      " 4.0       455\n",
      " 5.0       427\n",
      " 6.0       386\n",
      " 7.0       337\n",
      "Name: quality_2, dtype: int64\n",
      " 0.0    428096\n",
      "-1.0    153354\n",
      " 1.0     56171\n",
      " 2.0     35978\n",
      " 3.0     21596\n",
      " 4.0     11204\n",
      " 5.0      8536\n",
      " 6.0      6836\n",
      " 7.0      5506\n",
      " 8.0      4495\n",
      "Name: quality_5, dtype: int64\n",
      " 0      662217\n",
      "-1      153531\n",
      " 600      1852\n",
      " 1        1347\n",
      " 5         644\n",
      " 2         603\n",
      " 3         475\n",
      " 6         445\n",
      " 4         408\n",
      " 10        320\n",
      "Name: quality_6, dtype: int64\n",
      "0.0     757788\n",
      "1.0       6768\n",
      "2.0       3648\n",
      "5.0       3456\n",
      "6.0       2688\n",
      "3.0       2568\n",
      "4.0       2124\n",
      "10.0      1632\n",
      "11.0      1452\n",
      "7.0       1164\n",
      "Name: quality_7, dtype: int64\n",
      "0.0    787812\n",
      "1.0     19104\n",
      "2.0      8832\n",
      "3.0      4020\n",
      "4.0      2592\n",
      "5.0      1692\n",
      "6.0      1116\n",
      "7.0       744\n",
      "8.0       480\n",
      "9.0       384\n",
      "Name: quality_8, dtype: int64\n",
      "0.0    796284\n",
      "1.0      9024\n",
      "2.0      2772\n",
      "3.0      1800\n",
      "4.0      1416\n",
      "6.0      1176\n",
      "5.0      1152\n",
      "7.0       564\n",
      "8.0       420\n",
      "9.0       396\n",
      "Name: quality_9, dtype: int64\n",
      "3.0    99828\n",
      "2.0    81732\n",
      "0.0    59028\n",
      "1.0    58152\n",
      "4.0    54948\n",
      "5.0    40812\n",
      "6.0    32220\n",
      "7.0    24036\n",
      "8.0    19212\n",
      "9.0    15756\n",
      "Name: quality_10, dtype: int64\n",
      " 0     672229\n",
      "-1     153678\n",
      " 1       2428\n",
      " 2        203\n",
      " 3         50\n",
      " 4         19\n",
      " 5          8\n",
      " 6          5\n",
      " 14         1\n",
      " 9          1\n",
      "Name: quality_11, dtype: int64\n",
      "0     801156\n",
      "1      20880\n",
      "2       4524\n",
      "3       1320\n",
      "4        312\n",
      "5        192\n",
      "6        108\n",
      "8         48\n",
      "7         24\n",
      "14        12\n",
      "Name: quality_12, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    print(train_q['quality_{}'.format(i)].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from Error_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용한 model의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_count = train_e[['user_id', 'model_nm']].drop_duplicates().groupby('user_id').count()\n",
    "test_model_count = test_e[['user_id', 'model_nm']].drop_duplicates().groupby('user_id').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용한 fwver 의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fwver_count = train_e[['user_id', 'fwver']].drop_duplicates().groupby('user_id').count()\n",
    "test_fwver_count = test_e[['user_id', 'fwver']].drop_duplicates().groupby('user_id').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경험한 error 의 총 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err_count = train_e.groupby('user_id')['errcode'].count()\n",
    "test_err_count = test_e.groupby('user_id')['errcode'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경험한 각 errtype의 value별 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23호\n",
    "train_errcode_23 = train_e[train_e.errtype == 23][['user_id', 'errcode', 'hour']].groupby(['user_id', 'errcode']).count().unstack().fillna(0)\n",
    "test_errcode_23 = test_e[test_e.errtype == 23][['user_id', 'errcode', 'hour']].groupby(['user_id', 'errcode']).count().unstack().fillna(0)\n",
    "\n",
    "train_errcode_23.columns = ['UNKNOWN', 'ACTIVE', 'connLMP', 'connESTA', 'connTO', 'connLOCAL', 'STANDBY', 'TERMINATE']\n",
    "test_errcode_23.columns = ['UNKNOWN', 'ACTIVE', 'connLMP', 'connESTA', 'connTO', 'connLOCAL', 'STANDBY', 'TERMINATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31호\n",
    "train_errcode_31 = train_e[train_e.errtype == 31][['user_id', 'errcode', 'time']].\\\n",
    "                            groupby(['user_id', 'errcode']).count().unstack().fillna(0)\n",
    "test_errcode_31 = test_e[test_e.errtype == 31][['user_id', 'errcode', 'time']].\\\n",
    "                            groupby(['user_id', 'errcode']).count().unstack().fillna(0)\n",
    "train_errcode_31.columns = ['err_31_0', 'err_31_1']\n",
    "test_errcode_31.columns =['err_31_0', 'err_31_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33호\n",
    "train_errcode_33 = train_e[train_e.errtype == 33][['user_id', 'errcode', 'hour']].groupby(['user_id', 'errcode']).count().unstack().fillna(0)\n",
    "test_errcode_33 = test_e[test_e.errtype == 33][['user_id', 'errcode', 'hour']].groupby(['user_id', 'errcode']).count().unstack().fillna(0)\n",
    "\n",
    "train_errcode_33.columns = ['err_33_1', 'err_33_2', 'err_33_3']\n",
    "test_errcode_33.columns = ['err_33_1', 'err_33_2', 'err_33_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 34호\n",
    "train_errcode_34 = train_e[train_e.errtype == 34][['user_id', 'errcode', 'hour']].groupby(['user_id', 'errcode']).count().unstack().fillna(0)\n",
    "test_errcode_34 = test_e[test_e.errtype == 34][['user_id', 'errcode', 'hour']].groupby(['user_id', 'errcode']).count().unstack().fillna(0)\n",
    "\n",
    "train_errcode_34.columns = ['err_34_1', 'err_34_2', 'err_34_3', 'err_34_4', 'err_34_5', 'err_34_6']\n",
    "test_errcode_34.columns = ['err_34_1', 'err_34_2', 'err_34_3', 'err_34_4', 'err_34_5', 'err_34_6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### day max를 mean으로 나눈 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_e['day_2'] = train_e['time'].apply(lambda x: str(x)[:10])\n",
    "\n",
    "train_meanDay = (train_e\n",
    "                 .groupby(['user_id','day_2'])['day_2']\n",
    "                 .count()\n",
    "                 .unstack()\n",
    "                 .fillna(0)\n",
    "                 .loc[:, '2020-11-01':'2020-11-30']\n",
    "                 .mean(axis=1))\n",
    "\n",
    "train_maxDay = (train_e\n",
    "                .groupby(['user_id','day_2'])['day_2']\n",
    "                .count()\n",
    "                .unstack()\n",
    "                .fillna(0)\n",
    "                .loc[:, '2020-11-01':'2020-11-30']\n",
    "                .max(axis=1))\n",
    "\n",
    "train_maxBymean = pd.Series(data = np.array(train_maxDay) / np.array(train_meanDay),\n",
    "                            index = train_e.user_id.unique(),\n",
    "                            name = 'mbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_e['day_2'] = test_e['time'].apply(lambda x: str(x)[:10])\n",
    "\n",
    "test_meanDay = (test_e\n",
    "                .groupby(['user_id','day_2'])['day_2']\n",
    "                .count()\n",
    "                .unstack()\n",
    "                .fillna(0)\n",
    "                .loc[:, '2020-11-01':'2020-11-30']\n",
    "                .mean(axis=1))\n",
    "\n",
    "test_maxDay = (test_e\n",
    "               .groupby(['user_id','day_2'])['day_2']\n",
    "               .count()\n",
    "               .unstack()\n",
    "               .fillna(0)\n",
    "               .loc[:, '2020-11-01':'2020-11-30']\n",
    "               .max(axis=1))\n",
    "\n",
    "test_maxBymean = pd.Series(data = np.array(test_maxDay) / np.array(test_meanDay),\n",
    "                           index = test_e.user_id.unique(),\n",
    "                           name = 'mbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from Quality_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 quality의 표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qual_std = train_q.groupby(['user_id']).std()\n",
    "test_qual_std = test_q.groupby(['user_id']).std()\n",
    "\n",
    "new_columns = ['q_std_0', 'q_std_1', 'q_std_2', 'q_std_5', 'q_std_6', 'q_std_7', 'q_std_8', 'q_std_9', 'q_std_10','q_std_11', 'q_std_12']\n",
    "\n",
    "train_qual_std.columns = new_columns\n",
    "test_qual_std.columns = new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기록한 quality log의 개수(12개당 1번)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qual_log = train_q.groupby('user_id')['time'].count()/12\n",
    "test_qual_log = test_q.groupby('user_id')['time'].count()/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quality당 순수 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 값을 만들기 위해 새로운 df를 받아옵니다.\n",
    "train_q_temp = train_q.copy()\n",
    "test_q_temp = test_q.copy()\n",
    "\n",
    "# 0 값을 전부 nan 값으로 바꿔줍니다.\n",
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    train_q_temp.loc[train_q_temp['quality_{}'.format(i)] == 0, 'quality_{}'.format(i)] = np.nan\n",
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    test_q_temp.loc[test_q_temp['quality_{}'.format(i)] == 0, 'quality_{}'.format(i)] = np.nan\n",
    "\n",
    "# 필요없는 변수들을 버려줍니다.\n",
    "train_q_temp.drop(['time', 'fwver', 'year', 'month', 'day', 'hour', 'minute'], axis=1, inplace=True)\n",
    "test_q_temp.drop(['time', 'fwver', 'year', 'month', 'day', 'hour', 'minute'], axis=1, inplace=True)\n",
    "\n",
    "# 그룹바이 카운트 해줍니다.\n",
    "train_qual_counts = train_q_temp.groupby('user_id').count()\n",
    "test_qual_counts = test_q_temp.groupby('user_id').count()\n",
    "\n",
    "# 겹치는 컬럼명을 바꿔줍니다.\n",
    "train_qual_counts.columns = ['q_c_0', 'q_c_1', 'q_c_2', 'q_c_5', 'q_c_6',\n",
    "                             'q_c_7', 'q_c_8', 'q_c_9', 'q_c_10', 'q_c_11', 'q_c_12']\n",
    "test_qual_counts.columns = ['q_c_0', 'q_c_1', 'q_c_2', 'q_c_5', 'q_c_6',\n",
    "                             'q_c_7', 'q_c_8', 'q_c_9', 'q_c_10', 'q_c_11', 'q_c_12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quality당 음수, 0에 대한 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_temp = train_q.copy()\n",
    "test_q_temp = test_q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_temp.drop(['time', 'fwver', 'year', 'month', 'day', 'hour', 'minute'], axis=1, inplace=True)\n",
    "test_q_temp.drop(['time', 'fwver', 'year', 'month', 'day', 'hour', 'minute'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 값만 count를 위해서 음수와 양수를 전부 nan으로 만들겠습니다.\n",
    "\n",
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    train_q_temp.loc[train_q_temp['quality_{}'.format(i)] < 0, 'quality_{}'.format(i)] = np.nan\n",
    "    \n",
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    test_q_temp.loc[test_q_temp['quality_{}'.format(i)] < 0, 'quality_{}'.format(i)] = np.nan\n",
    "    \n",
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    train_q_temp.loc[train_q_temp['quality_{}'.format(i)] > 0, 'quality_{}'.format(i)] = np.nan\n",
    "    \n",
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    test_q_temp.loc[test_q_temp['quality_{}'.format(i)] > 0, 'quality_{}'.format(i)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qual_zeroCount = train_q_temp.groupby('user_id').count()\n",
    "test_qual_zeroCount = test_q_temp.groupby('user_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['q_z_c_0', 'q_z_c_1', 'q_z_c_2', 'q_z_c_5', 'q_z_c_6', 'q_z_c_7', 'q_z_c_8', 'q_z_c_9', 'q_z_c_10','q_z_c_11', 'q_z_c_12']\n",
    "\n",
    "train_qual_zeroCount.columns = new_columns\n",
    "test_qual_zeroCount.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_temp = train_q.copy()\n",
    "test_q_temp = test_q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q_temp.drop(['time', 'fwver', 'year', 'month', 'day', 'hour', 'minute'], axis=1, inplace=True)\n",
    "test_q_temp.drop(['time', 'fwver', 'year', 'month', 'day', 'hour', 'minute'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음수 값만 count를 위해서 음수와 0을 전부 nan으로 만들겠습니다.\n",
    "\n",
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    train_q_temp.loc[train_q_temp['quality_{}'.format(i)] >= 0, 'quality_{}'.format(i)] = np.nan\n",
    "    \n",
    "for i in [0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "    test_q_temp.loc[test_q_temp['quality_{}'.format(i)] >= 0, 'quality_{}'.format(i)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qual_negaCount = train_q_temp.groupby('user_id').count()\n",
    "test_qual_negaCount = test_q_temp.groupby('user_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['q_n_c_0', 'q_n_c_1', 'q_n_c_2', 'q_n_c_5', 'q_n_c_6', 'q_n_c_7', 'q_n_c_8', 'q_n_c_9', 'q_n_c_10','q_n_c_11', 'q_n_c_12']\n",
    "\n",
    "train_qual_negaCount.columns = new_columns\n",
    "test_qual_negaCount.columns = new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quality를 sum으로 groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_each_quality_sum = train_q.groupby('user_id').sum().loc[:, 'quality_0':'quality_12']\n",
    "test_each_quality_sum = test_q.groupby('user_id').sum().loc[:, 'quality_0':'quality_12']\n",
    "\n",
    "quality_sum_colnms = ['quality_0_sum', 'quality_1_sum', 'quality_2_sum', 'quality_5_sum', 'quality_6_sum', \n",
    "                      'quality_7_sum', 'quality_8_sum', 'quality_9_sum','quality_10_sum', 'quality_11_sum', \n",
    "                      'quality_12_sum']\n",
    "\n",
    "train_each_quality_sum.columns = quality_sum_colnms\n",
    "test_each_quality_sum.columns = quality_sum_colnms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time에 대한 유저별 표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_seconds(x):\n",
    "    return time.mktime(x.timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_e['time_sec'] = train_e.time.apply(lambda x: time_to_seconds(x))\n",
    "test_e['time_sec'] = test_e.time.apply(lambda x: time_to_seconds(x))\n",
    "train_q['time_sec'] = train_q.time.apply(lambda x: time_to_seconds(x))\n",
    "test_q['time_sec'] = test_q.time.apply(lambda x: time_to_seconds(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 471 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_err_timestd = train_e.groupby(['user_id'])['time_sec'].std()\n",
    "test_err_timestd = test_e.groupby(['user_id'])['time_sec'].std()\n",
    "train_err_timestd = train_err_timestd.rename(level = 0, index = 't_e_std') \n",
    "test_err_timestd = test_err_timestd.rename(level = 0, index = 't_e_std') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_qual_timestd = (train_q[['user_id', 'time_sec']].drop_duplicates()).groupby(['user_id']).std()\n",
    "test_qual_timestd = (test_q[['user_id', 'time_sec']].drop_duplicates()).groupby(['user_id']).std()\n",
    "train_qual_timestd.columns = ['t_q_std']\n",
    "test_qual_timestd.columns = ['t_q_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errtype을 유저별로 카운트 해줍니다.\n",
    "\n",
    "X = train_e.groupby(['user_id', 'errtype'])['errcode'].count().unstack().fillna(0)\n",
    "y = test_e.groupby(['user_id', 'errtype'])['errcode'].count().unstack().fillna(0)\n",
    "\n",
    "X.columns = ['et_1', 'et_2', 'et_3', 'et_4', 'et_5', 'et_6', 'et_7', 'et_8', 'et_9', 'et_10', 'et_11', 'et_12', 'et_13',\n",
    " 'et_14', 'et_15', 'et_16', 'et_17', 'et_18', 'et_19', 'et_20', 'et_21', 'et_22', 'et_23', 'et_24', 'et_25',\n",
    " 'et_26', 'et_27', 'et_28', 'et_30', 'et_31', 'et_32', 'et_33', 'et_34', 'et_35', 'et_36', 'et_37', 'et_38',\n",
    " 'et_39', 'et_40', 'et_41', 'et_42']\n",
    "y.columns = ['et_1', 'et_2', 'et_3', 'et_4', 'et_5', 'et_6', 'et_7', 'et_8', 'et_9', 'et_10', 'et_11', 'et_12', 'et_13',\n",
    " 'et_14', 'et_15', 'et_16', 'et_17', 'et_18', 'et_19', 'et_20', 'et_21', 'et_22', 'et_23', 'et_24', 'et_25',\n",
    " 'et_26', 'et_27', 'et_28', 'et_30', 'et_31', 'et_32', 'et_33', 'et_34', 'et_35', 'et_36', 'et_37', 'et_38',\n",
    " 'et_39', 'et_40', 'et_41', 'et_42']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나 사라진 유저를 채워주는 코드입니다.\n",
    "y = y.reindex(pd.RangeIndex(y.index.max() + 1)).ffill(0)[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 41), (14999, 41))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 122)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X,\n",
    "               train_err_count, # 유저가 기록한 총 err수\n",
    "               train_fwver_count, # 유저가 사용한 fw수\n",
    "               train_model_count, # 유저가 사용한 model 수\n",
    "               train_qual_std, # 각 퀄리티에 대한 유저별 편차\n",
    "               train_qual_log, # 유저별 퀄리티 로그의 수\n",
    "               train_errcode_23, # 23호 에러타입의 밸류별 개수\n",
    "               train_errcode_33, # 33호 상동\n",
    "               train_errcode_34, # 34호 상동\n",
    "               train_qual_counts, # 각 퀄리티에서 0을 제외한 카운트\n",
    "               train_qual_negaCount, # 각 퀄리티에 대해 음수만 카운트\n",
    "               train_qual_zeroCount, # 각 퀄리티에 대해 0.만 카운트\n",
    "               train_err_timestd,\n",
    "               train_qual_timestd,\n",
    "               train_each_quality_sum,\n",
    "               train_errcode_31, # 31호 상동\n",
    "               train_maxBymean # time\n",
    "              ], axis=1).fillna(0)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14999, 122)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.concat([y,\n",
    "               test_err_count, # 유저가 기록한 총 err수\n",
    "               test_fwver_count, # 유저가 사용한 fw수\n",
    "               test_model_count, # 유저가 사용한 model 수\n",
    "               test_qual_std, # 각 퀄리티에 대한 유저별 편차\n",
    "               test_qual_log, # 유저별 퀄리티 로그의 수\n",
    "               test_errcode_23, # 23호 에러타입의 밸류별 개수\n",
    "               test_errcode_33, # 33호 상동\n",
    "               test_errcode_34, # 34호 상동\n",
    "               test_qual_counts, # 각 퀄리티에서 0을 제외한 카운트\n",
    "               test_qual_negaCount, # 각 퀄리티에 대해 음수만 카운트\n",
    "               test_qual_zeroCount, # 각 퀄리티에 대해 0.만 카운트\n",
    "               test_err_timestd,\n",
    "               test_qual_timestd,\n",
    "               test_each_quality_sum,\n",
    "               test_errcode_31, # 31호 상동\n",
    "               test_maxBymean, # time\n",
    "              ], axis=1).fillna(0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 120), (14999, 120))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일부 다중공선성을 띄는 변수들을 제거해줍니다.\n",
    "X.drop(['et_20', 'et_36'], axis=1, inplace = True)\n",
    "y.drop(['et_20', 'et_36'], axis=1, inplace = True)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Catb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catb_fold_train_pred(train_x, train_y):\n",
    "\n",
    "    # Train\n",
    "    models     = []\n",
    "    recalls    = []\n",
    "    precisions = []\n",
    "    auc_scores   = []\n",
    "    threshold = 0.5\n",
    "    # 파라미터 설정\n",
    "    params =      {\n",
    "                    'nan_mode': 'Min',\n",
    "                    'eval_metric': 'Logloss',\n",
    "                    'iterations': 1000,\n",
    "                    'sampling_frequency': 'PerTree',\n",
    "                    'leaf_estimation_method': 'Newton',\n",
    "                    'grow_policy': 'SymmetricTree',\n",
    "                    'penalties_coefficient': 1,\n",
    "                    'boosting_type': 'Plain',\n",
    "                    'model_shrink_mode': 'Constant',\n",
    "                    'feature_border_type': 'GreedyLogSum',\n",
    "                    'l2_leaf_reg': 3,\n",
    "                    'random_strength': 1,\n",
    "                    'rsm': 1,\n",
    "                    'boost_from_average': False,\n",
    "                    'model_size_reg': 0.5,\n",
    "                    'subsample': 0.800000011920929,\n",
    "                    'use_best_model': False,\n",
    "                    'class_names': [0, 1],\n",
    "                    'random_seed': 2584,\n",
    "                    'depth': 6,\n",
    "                    'posterior_sampling': False,\n",
    "                    'border_count': 254,\n",
    "                    'classes_count': 0,\n",
    "                    'auto_class_weights': 'None',\n",
    "                    'sparse_features_conflict_fraction': 0,\n",
    "                    'leaf_estimation_backtracking': 'AnyImprovement',\n",
    "                    'best_model_min_trees': 1,\n",
    "                    'model_shrink_rate': 0,\n",
    "                    'min_data_in_leaf': 1,\n",
    "                    'loss_function': 'Logloss',\n",
    "                    'learning_rate': 0.028116999194025993,\n",
    "                    'score_function': 'Cosine',\n",
    "                    'task_type': 'CPU',\n",
    "                    'leaf_estimation_iterations': 10,\n",
    "                    'bootstrap_type': 'MVS',\n",
    "                    'max_leaves': 64\n",
    "                    }\n",
    "    #-------------------------------------------------------------------------------------\n",
    "    # 5 Kfold cross validation\n",
    "    s_fold = StratifiedKFold(n_splits=5, shuffle=True ,random_state=42)    \n",
    "\n",
    "    for train_idx, val_idx in s_fold.split(train_x, train_y):\n",
    "\n",
    "        # split train, validation set\n",
    "        X = train_x.iloc[train_idx]\n",
    "        y = train_y.iloc[train_idx]\n",
    "        valid_x = train_x.iloc[val_idx]\n",
    "        valid_y = train_y.iloc[val_idx]\n",
    "\n",
    "        #run traning\n",
    "        model = CatBoostClassifier(**params, verbose=0)\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # cal valid prediction\n",
    "        valid_prob = model.predict(valid_x)\n",
    "        valid_pred = np.where(valid_prob > threshold, 1, 0)\n",
    "\n",
    "        # cal scores\n",
    "        recall    = recall_score(    valid_y, valid_pred)\n",
    "        precision = precision_score( valid_y, valid_pred)\n",
    "        auc_score = roc_auc_score(   valid_y, valid_prob)\n",
    "\n",
    "        # append scores\n",
    "        models.append(model)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "    return models, auc_scores, recalls, precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7301499999999999\n"
     ]
    }
   ],
   "source": [
    "# loss 비교를 위해 지우지 않습니다.\n",
    "# 최고점 모델입니다.\n",
    "models, auc_scores, _, _ = catb_fold_train_pred(X, train_b_p.target)\n",
    "print(np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_list = []\n",
    "for model in models:\n",
    "    pred_y = model.predict_proba(y)\n",
    "    pred_y_list.append(pred_y[:, 1].reshape(-1,1))\n",
    "    \n",
    "pred_ensemble_catb = np.mean(pred_y_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93218016],\n",
       "       [0.22254721],\n",
       "       [0.48239791],\n",
       "       ...,\n",
       "       [0.70990508],\n",
       "       [0.8953148 ],\n",
       "       [0.43733988]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ensemble_catb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_fold_train_pred(train_x, train_y):\n",
    "\n",
    "    # Train\n",
    "    models     = []\n",
    "    recalls    = []\n",
    "    precisions = []\n",
    "    auc_scores   = []\n",
    "    threshold = 0.5\n",
    "    # 파라미터 설정\n",
    "    params =      {\n",
    "                    'boosting_type':'gbdt', \n",
    "                    'class_weight':None,\n",
    "                    'colsample_bytree':1.0,\n",
    "                    'importance_type':'split',\n",
    "                    'learning_rate':0.1,\n",
    "                    'max_depth':-1,\n",
    "                    'min_child_samples':20,\n",
    "                    'min_child_weight':0.001,\n",
    "                    'min_split_gain':0.0,\n",
    "                    'n_estimators':100,\n",
    "                    'n_jobs':-1,\n",
    "                    'num_leaves':31,\n",
    "                    'objective':None,\n",
    "                    'random_state':2584,\n",
    "                    'reg_alpha':0.0,\n",
    "                    'reg_lambda':0.0,\n",
    "                    'silent':True,\n",
    "                    'subsample':1.0,\n",
    "                    'subsample_for_bin':200000,\n",
    "                    'subsample_freq':0\n",
    "                    }\n",
    "    #-------------------------------------------------------------------------------------\n",
    "    # 5 Kfold cross validation\n",
    "    s_fold = StratifiedKFold(n_splits=5, shuffle=True ,random_state=42)    \n",
    "\n",
    "    for train_idx, val_idx in s_fold.split(train_x, train_y):\n",
    "\n",
    "        # split train, validation set\n",
    "        X = train_x.iloc[train_idx]\n",
    "        y = train_y.iloc[train_idx]\n",
    "        valid_x = train_x.iloc[val_idx]\n",
    "        valid_y = train_y.iloc[val_idx]\n",
    "\n",
    "        #run traning\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # cal valid prediction\n",
    "        valid_prob = model.predict(valid_x)\n",
    "        valid_pred = np.where(valid_prob > threshold, 1, 0)\n",
    "\n",
    "        # cal scores\n",
    "        recall    = recall_score(    valid_y, valid_pred)\n",
    "        precision = precision_score( valid_y, valid_pred)\n",
    "        auc_score = roc_auc_score(   valid_y, valid_prob)\n",
    "\n",
    "        # append scores\n",
    "        models.append(model)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        auc_scores.append(auc_score)\n",
    "        \n",
    "    return models, auc_scores, recalls, precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7339499999999999\n"
     ]
    }
   ],
   "source": [
    "models, auc_scores, _, _ = lgbm_fold_train_pred(X, train_b_p.target)\n",
    "print(np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_list = []\n",
    "for model in models:\n",
    "    pred_y = model.predict_proba(y)\n",
    "    pred_y_list.append(pred_y[:, 1].reshape(-1,1))\n",
    "    \n",
    "pred_ensemble_lgbm = np.mean(pred_y_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94646884],\n",
       "       [0.23926972],\n",
       "       [0.47741895],\n",
       "       ...,\n",
       "       [0.78794031],\n",
       "       [0.89074742],\n",
       "       [0.42286365]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ensemble_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(\n",
    "                                ccp_alpha=0.0,\n",
    "                                criterion='friedman_mse',\n",
    "                                init=None,\n",
    "                                learning_rate=0.1,\n",
    "                                loss='deviance',\n",
    "                                max_depth=3,\n",
    "                                max_features=None,\n",
    "                                max_leaf_nodes=None,\n",
    "                                min_impurity_decrease=0.0,\n",
    "                                min_impurity_split=None,\n",
    "                                min_samples_leaf=1,\n",
    "                                min_samples_split=2,\n",
    "                                min_weight_fraction_leaf=0.0,\n",
    "                                n_estimators=100,\n",
    "                                n_iter_no_change=None,\n",
    "                                presort='deprecated',\n",
    "                                random_state=2584,\n",
    "                                subsample=1.0,\n",
    "                                tol=0.0001,\n",
    "                                validation_fraction=0.1,\n",
    "                                verbose=0,\n",
    "                                warm_start=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbc_fold_train_pred(train_x, train_y, N_SPLIT=5):\n",
    "\n",
    "    # Train\n",
    "    models     = []\n",
    "    recalls    = []\n",
    "    precisions = []\n",
    "    auc_scores   = []\n",
    "    threshold = 0.5\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=N_SPLIT, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in k_fold.split(train_x, train_y):\n",
    "\n",
    "        # split train, validation set\n",
    "        X = train_x.iloc[train_idx]\n",
    "        y = train_y.iloc[train_idx]\n",
    "        valid_x = train_x.iloc[val_idx]\n",
    "        valid_y = train_y.iloc[val_idx]\n",
    "\n",
    "        #run traning\n",
    "        model = gbc.fit(X, y)\n",
    "\n",
    "        # cal valid prediction\n",
    "        valid_prob = model.predict(valid_x)\n",
    "        valid_pred = np.where(valid_prob > threshold, 1, 0)\n",
    "\n",
    "        # cal scores\n",
    "        recall    = recall_score(    valid_y, valid_pred)\n",
    "        precision = precision_score( valid_y, valid_pred)\n",
    "        auc_score = roc_auc_score(   valid_y, valid_prob)\n",
    "        \n",
    "        # append scores\n",
    "        models.append(model)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        auc_scores.append(auc_score)\n",
    "    \n",
    "    return models, auc_scores, recalls, precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72455\n",
      "Wall time: 36.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "models, auc_scores, _, _ = gbc_fold_train_pred(X, train_b_p.target)\n",
    "print(np.mean(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_list = []\n",
    "for model in models:\n",
    "    pred_y = model.predict_proba(y)\n",
    "    pred_y_list.append(pred_y[:, 1].reshape(-1,1))\n",
    "    \n",
    "pred_ensemble_gbc = np.mean(pred_y_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94729912],\n",
       "       [0.20899576],\n",
       "       [0.48470505],\n",
       "       ...,\n",
       "       [0.75559137],\n",
       "       [0.87481054],\n",
       "       [0.34822194]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ensemble_gbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93218016],\n",
       "       [0.22254721],\n",
       "       [0.48239791],\n",
       "       ...,\n",
       "       [0.70990508],\n",
       "       [0.8953148 ],\n",
       "       [0.43733988]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ensemble_catb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94646884],\n",
       "       [0.23926972],\n",
       "       [0.47741895],\n",
       "       ...,\n",
       "       [0.78794031],\n",
       "       [0.89074742],\n",
       "       [0.42286365]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ensemble_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94729912],\n",
       "       [0.20899576],\n",
       "       [0.48470505],\n",
       "       ...,\n",
       "       [0.75559137],\n",
       "       [0.87481054],\n",
       "       [0.34822194]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ensemble_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>problem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000</td>\n",
       "      <td>0.941983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30001</td>\n",
       "      <td>0.223604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30002</td>\n",
       "      <td>0.481507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30003</td>\n",
       "      <td>0.790742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30004</td>\n",
       "      <td>0.897451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14994</th>\n",
       "      <td>44994</td>\n",
       "      <td>0.184165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>44995</td>\n",
       "      <td>0.289131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>44996</td>\n",
       "      <td>0.751146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>44997</td>\n",
       "      <td>0.886958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>44998</td>\n",
       "      <td>0.402808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id   problem\n",
       "0        30000  0.941983\n",
       "1        30001  0.223604\n",
       "2        30002  0.481507\n",
       "3        30003  0.790742\n",
       "4        30004  0.897451\n",
       "...        ...       ...\n",
       "14994    44994  0.184165\n",
       "14995    44995  0.289131\n",
       "14996    44996  0.751146\n",
       "14997    44997  0.886958\n",
       "14998    44998  0.402808\n",
       "\n",
       "[14999 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ensemble_best = (\n",
    "                            pred_ensemble_catb +\n",
    "                            pred_ensemble_lgbm  +\n",
    "                            pred_ensemble_gbc\n",
    "                        ) / 3\n",
    "\n",
    "submission.problem = pred_ensemble_best\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "끝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
